{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Hacker's Guite to Neural Networks](http://karpathy.github.io/neuralnets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardMultiplyGate(a, b):\n",
    "    return a * b;\n",
    "\n",
    "def forwardAddGate(a, b):\n",
    "    return a + b\n",
    "\n",
    "def forwardCircuit(x,y,z):\n",
    "    q = forwardAddGate(x, y)\n",
    "    f = forwardMultiplyGate(q, z)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "# initial conditions\n",
    "x = -2\n",
    "y = 5\n",
    "z = -4\n",
    "q = forwardAddGate(x, y); # q is 3\n",
    "f = forwardMultiplyGate(q, z) # output is -12\n",
    "\n",
    "# gradient of the MULTIPLY gate with respect to its inputs\n",
    "# wrt is short for \"with respect to\"\n",
    "derivative_f_wrt_z = q\n",
    "derivative_f_wrt_q = z\n",
    "\n",
    "# derivative of the ADD gate with respect to its inputs\n",
    "derivative_q_wrt_x = 1.0\n",
    "derivative_q_wrt_y = 1.0\n",
    "\n",
    "# chain rule\n",
    "derivative_f_wrt_x = derivative_q_wrt_x * derivative_f_wrt_q\n",
    "derivative_f_wrt_y = derivative_q_wrt_y * derivative_f_wrt_q\n",
    "\n",
    "\n",
    "# final gradient, from above: [-4, -4, 3]\n",
    "gradient_f_wrt_xyz = [derivative_f_wrt_x, derivative_f_wrt_y, derivative_f_wrt_z]\n",
    "\n",
    "# let the inputs respond to the force/tug:\n",
    "step_size = 0.01;\n",
    "x = x + step_size * derivative_f_wrt_x\n",
    "y = y + step_size * derivative_f_wrt_y\n",
    "z = z + step_size * derivative_f_wrt_z\n",
    "\n",
    "\n",
    "q = forwardAddGate(x, y) \n",
    "f = forwardMultiplyGate(q, z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x, y, a, b, c)=\\sigma(ax+by+c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "import math\n",
    "\n",
    "# every Unit corresponds to a wire in the diagrams\n",
    "class Unit(object):\n",
    "    def __init__(self, value, grad):\n",
    "        # value computed in the forward pass\n",
    "        self.value = value\n",
    "        # the derivative of circuit output w.r.t this unit, computed in backward pass\n",
    "        self.grad = grad\n",
    "\n",
    "\n",
    "class ABCGate(ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def forward(self):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def backward(self):\n",
    "        pass\n",
    "        \n",
    "\n",
    "class MultiplyGate(ABCGate):\n",
    "    \n",
    "    def forward(self, u0, u1):\n",
    "        # store pointers to input Units u0 and u1 and output unit utop\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value * u1.value, 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        # take the gradient in output unit and chain it with the\n",
    "        # local gradients, which we derived for multiply gate before\n",
    "        # then write those gradients to those Units.\n",
    "        self.u0.grad += self.u1.value * self.utop.grad\n",
    "        self.u1.grad += self.u0.value * self.utop.grad;\n",
    "\n",
    "\n",
    "class AddGate(ABCGate):\n",
    "    \n",
    "    def forward(self, u0, u1):\n",
    "        self.u0 = u0\n",
    "        self.u1 = u1\n",
    "        self.utop = Unit(u0.value + u1.value, 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        # add gate. derivative wrt both inputs is 1\n",
    "        self.u0.grad += 1 * self.utop.grad\n",
    "        self.u1.grad += 1 * self.utop.grad\n",
    "\n",
    "class SigmoidGate(ABCGate):\n",
    "    \n",
    "    # helper function\n",
    "    def _sig(self, x):\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    \n",
    "    def forward(self, u0):\n",
    "        self.u0 = u0\n",
    "        self.utop = Unit(self._sig(self.u0.value), 0.0)\n",
    "        return self.utop\n",
    "    \n",
    "    def backward(self):\n",
    "        s = self._sig(self.u0.value)\n",
    "        self.u0.grad += (s * (1 - s)) * self.utop.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circuit output: 0.8807970779778823\n"
     ]
    }
   ],
   "source": [
    "# create input units\n",
    "a = Unit(1.0, 0.0)\n",
    "b = Unit(2.0, 0.0)\n",
    "c = Unit(-3.0, 0.0)\n",
    "x = Unit(-1.0, 0.0)\n",
    "y = Unit(3.0, 0.0)\n",
    "\n",
    "# create the gates\n",
    "mulg0 = MultiplyGate()\n",
    "mulg1 = MultiplyGate()\n",
    "addg0 = AddGate()\n",
    "addg1 = AddGate()\n",
    "sg0 = SigmoidGate()\n",
    "\n",
    "# do the forward pass\n",
    "def forward_neuron():\n",
    "    ax = mulg0.forward(a, x) # a*x = -1\n",
    "    by = mulg1.forward(b, y) # b*y = 6\n",
    "    axpby = addg0.forward(ax, by) # a*x + b*y = 5\n",
    "    axpbypc = addg1.forward(axpby, c) # a*x + b*y + c = 2\n",
    "    s = sg0.forward(axpbypc) # sig(a*x + b*y + c) = 0.8808\n",
    "    return s\n",
    "\n",
    "s = forward_neuron()\n",
    "\n",
    "print('circuit output: {0.value}'.format(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Gradient\n",
    "s.grad = 1.0;\n",
    "sg0.backward() # writes gradient into axpbypc\n",
    "addg1.backward() # writes gradients into axpby and c\n",
    "addg0.backward() # writes gradients into ax and by\n",
    "mulg1.backward() # writes gradients into b and y\n",
    "mulg0.backward() # writes gradients into a and x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10499358540350662"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "circuit output after one backprop: 0.8825501816218984\n"
     ]
    }
   ],
   "source": [
    "step_size = 0.01\n",
    "\n",
    "a.value += step_size * a.grad # a.grad is -0.105\n",
    "b.value += step_size * b.grad # b.grad is 0.315\n",
    "c.value += step_size * c.grad # c.grad is 0.105\n",
    "x.value += step_size * x.grad # x.grad is 0.105\n",
    "y.value += step_size * y.grad # y.grad is 0.210\n",
    "\n",
    "s = forward_neuron();\n",
    "print('circuit output after one backprop: {s.value}'.format(s=s)) # prints 0.8825"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x,y,a,b,c)=ax+by+c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We select a random datapoint and feed it through the circuit\n",
    "\n",
    "2. We will interpret the output of the circuit as a confidence that the datapoint has class +1. (i.e. very high values = circuit is very certain datapoint has class +1 and very low values = circuit is certain this datapoint has class -1.)\n",
    "\n",
    "3. We will measure how well the prediction aligns with the provided labels. Intuitively, for example, if a positive example scores very low, we will want to tug in the positive direction on the circuit, demanding that it should output higher value for this datapoint. Note that this is the case for the the first datapoint: it is labeled as +1 but our predictor unction only assigns it value -1.2. We will therefore tug on the circuit in positive direction; We want the value to be higher.\n",
    "\n",
    "4. The circuit will take the tug and backpropagate it to compute tugs on the inputs a,b,c,x,y\n",
    "\n",
    "5. Since we think of x,y as (fixed) datapoints, we will ignore the pull on x,y. If you’re a fan of my physical analogies, think of these inputs as pegs, fixed in the ground.\n",
    "\n",
    "6. On the other hand, we will take the parameters a,b,c and make them respond to their tug (i.e. we’ll perform what we call a parameter update). This, of course, will make it so that the circuit will output a slightly higher score on this particular datapoint in the future.\n",
    "\n",
    "7. Iterate! Go back to step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [([1.2, 0.7], +1),\n",
    "        ([-0.3, 0.5], -1),\n",
    "        ([-3, -1], +1),\n",
    "        ([0.1, 1.0], -1),\n",
    "        ([3.0, 1.1], -1),\n",
    "        ([2.1, -3], +1)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A circuit: it takes 5 Units (x,y,a,b,c) and outputs a single Unit\n",
    "# It can also compute the gradient w.r.t. its inputs\n",
    "class Circuit(ABCGate):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # create some gates\n",
    "        self.mulg0 = MultiplyGate()\n",
    "        self.mulg1 = MultiplyGate()\n",
    "        self.addg0 = AddGate()\n",
    "        self.addg1 = AddGate()\n",
    "        \n",
    "    def forward(self, x,y,a,b,c):\n",
    "        self.ax = self.mulg0.forward(a, x) # a*x\n",
    "        self.by = self.mulg1.forward(b, y) # b*y\n",
    "        self.axpby = self.addg0.forward(self.ax, self.by) # a*x + b*y\n",
    "        self.axpbypc = self.addg1.forward(self.axpby, c) # a*x + b*y + c\n",
    "        return self.axpbypc\n",
    "    \n",
    "    def backward(self, gradient_top): # takes pull from above\n",
    "        self.axpbypc.grad = gradient_top\n",
    "        self.addg1.backward() # sets gradient in axpby and c\n",
    "        self.addg0.backward() # sets gradient in ax and by\n",
    "        self.mulg1.backward() # sets gradient in b and y\n",
    "        self.mulg0.backward() # sets gradient in a and x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM class\n",
    "\n",
    "class SVM(ABCGate):\n",
    "\n",
    "    def __init__(self):\n",
    "        # random initial parameter values\n",
    "        self.a = Unit(1.0, 0.0)\n",
    "        self.b = Unit(-2.0, 0.0)\n",
    "        self.c = Unit(-1.0, 0.0)\n",
    "        \n",
    "        self.circuit = Circuit()\n",
    "    \n",
    "    def forward(self, x, y): # assume x and y are Units\n",
    "        self.unit_out = self.circuit.forward(x, y, self.a, self.b, self.c)\n",
    "        return self.unit_out\n",
    "    \n",
    "    def backward(self, label): # label is +1 or -1\n",
    "        # reset pulls on a,b,c\n",
    "        self.a.grad = 0.0\n",
    "        self.b.grad = 0.0\n",
    "        self.c.grad = 0.0\n",
    "        \n",
    "        # compute the pull based on what the circuit output was\n",
    "        pull = 0.0 # this could be more flexible...\n",
    "        if (label == 1) & (self.unit_out.value < 1):\n",
    "            pull = 1 # the score was too low: pull up\n",
    "        elif (label == -1) & (self.unit_out.value > -1):\n",
    "            pull = -1 # the score was too high for a positive example, pull down\n",
    "        \n",
    "        self.circuit.backward(pull) # writes gradient into x,y,a,b,c\n",
    "        \n",
    "        # add regularization pull for parameters: towards zero and proportional to value\n",
    "        self.a.grad -= self.a.value\n",
    "        self.b.grad -= self.b.value\n",
    "    \n",
    "    def learn_from(self, x, y, label):\n",
    "        self.forward(x, y) # forward pass (set .value in all Units)\n",
    "        self.backward(label) # backward pass (set .grad in all Units)\n",
    "        self.parameter_update() # parameters respond to tug\n",
    "    \n",
    "    def parameter_update(self):\n",
    "        step_size = 0.01\n",
    "        self.a.value += step_size * self.a.grad\n",
    "        self.b.value += step_size * self.b.grad\n",
    "        self.c.value += step_size * self.c.grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy at iter 0: 0.6666666666666666\n",
      "training accuracy at iter 25: 0.6666666666666666\n",
      "training accuracy at iter 50: 0.8333333333333334\n",
      "training accuracy at iter 75: 0.8333333333333334\n",
      "training accuracy at iter 100: 0.8333333333333334\n",
      "training accuracy at iter 125: 0.8333333333333334\n",
      "training accuracy at iter 150: 0.8333333333333334\n",
      "training accuracy at iter 175: 0.8333333333333334\n",
      "training accuracy at iter 200: 0.8333333333333334\n",
      "training accuracy at iter 225: 0.8333333333333334\n",
      "training accuracy at iter 250: 0.8333333333333334\n",
      "training accuracy at iter 275: 0.8333333333333334\n",
      "training accuracy at iter 300: 0.8333333333333334\n",
      "training accuracy at iter 325: 0.8333333333333334\n",
      "training accuracy at iter 350: 1.0\n",
      "training accuracy at iter 375: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = [[1.2, 0.7], [-0.3, -0.5], [3.0, 0.1],\n",
    "        [-0.1, -1.0], [-1.0, 1.1], [2.1, -3]]\n",
    "labels = [1, -1, 1, -1, -1, 1]\n",
    "\n",
    "svm = SVM()\n",
    "\n",
    "# a function that computes the classification accuracy\n",
    "def eval_training_accuracy():\n",
    "    num_correct = 0\n",
    "    for i in range(len(data)):\n",
    "        x = Unit(data[i][0], 0.0)\n",
    "        y = Unit(data[i][1], 0.0)\n",
    "        true_label = labels[i]\n",
    "        \n",
    "        # see if the prediction matches the provided label\n",
    "        if svm.forward(x, y).value > 0:\n",
    "            predicted_label = 1 \n",
    "        else:\n",
    "            predicted_label = -1\n",
    "        \n",
    "        if predicted_label == true_label:\n",
    "            num_correct+=1\n",
    "\n",
    "    return num_correct / float(len(data));\n",
    "\n",
    "\n",
    "# the learning loop\n",
    "for iter in range(400):\n",
    "    # pick a random data point\n",
    "    i = np.random.randint(len(data), size=1)[0]\n",
    "    x = Unit(data[i][0], 0.0)\n",
    "    y = Unit(data[i][1], 0.0)\n",
    "    label = labels[i]\n",
    "    svm.learn_from(x, y, label)\n",
    "    \n",
    "    if iter % 25 == 0: #/ every 10 iterations... \n",
    "        print('training accuracy at iter {iter}: {ac}'.format(iter=iter,\n",
    "                                                             ac=eval_training_accuracy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
